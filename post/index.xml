<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Mai Le</title>
    <link>https://lephanthuymai.github.io/post/</link>
      <atom:link href="https://lephanthuymai.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â©2021 by Mai Le.</copyright><lastBuildDate>Tue, 02 Mar 2021 20:55:42 +0700</lastBuildDate>
    <image>
      <url>https://lephanthuymai.github.io/images/icon_huecf101d77c4d50fee89da6355e975757_514918_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://lephanthuymai.github.io/post/</link>
    </image>
    
    <item>
      <title>Hypothesis Testing Simply Explained</title>
      <link>https://lephanthuymai.github.io/post/hypothesis-testing-simply-explained/</link>
      <pubDate>Tue, 02 Mar 2021 20:55:42 +0700</pubDate>
      <guid>https://lephanthuymai.github.io/post/hypothesis-testing-simply-explained/</guid>
      <description>&lt;p&gt;When I was working for a software outsourcing firm, we had performance appraisal every six months. A lengthy process was applied to ensure fair recognition but there were always rumors on how employees reporting under certain departments or even under certain managers tend to have higher chances of promotion. The HR department made some simple calculations like averaging ratings of members under each manager/each department then compared the results, but is it enough to use that as an indicator that one department&amp;rsquo;s managers are easier on their employees than another? Similar problems in management when you need to validate a theory or a bias, can be addressed using &lt;strong&gt;hypothesis testing&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;what-is-hypothesis-testing-then&#34;&gt;What is hypothesis testing then?&lt;/h2&gt;
&lt;img src=&#34;https://www.kdnuggets.com/wp-content/uploads/xkcd-p-value-jellybeans.jpg&#34; width=&#34;400&#34;/&gt;
&lt;p&gt;&lt;em&gt;Source: &lt;a href=&#34;https://xkcd.com/882/&#34;&gt;https://xkcd.com/882/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A hypothesis test is a statistical test designed to verify if there is enough evidence in the data sample to reject or not reject an assumption. This assumption is called the &lt;strong&gt;null hypothesis&lt;/strong&gt;, it is the status quo, meaning there is no difference among test groups, nothing interesting happens, any observed differences are by sampling or by errors. When examining the data and finding enough &amp;ldquo;evidence&amp;rdquo;, we can reject the null hypothesis and accept an &lt;strong&gt;alternative hypothesis&lt;/strong&gt;. In the example of performance appraisal rating above, the null hypothesis states that there is no difference in the way different departments rate their employees, whereas the alternative hypothesis states that there is indeed difference in ratings among departments.&lt;/p&gt;
&lt;h2 id=&#34;how-can-we-find-evidence-to-reject-the-null-hypothesis&#34;&gt;How can we find &amp;ldquo;evidence&amp;rdquo; to reject the null hypothesis?&lt;/h2&gt;
&lt;p&gt;This is where things get more complicated: we need to define and compute a &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Test_statistic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;test statistic&lt;/a&gt;&lt;/strong&gt; and a &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/P-value&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-value&lt;/a&gt;&lt;/strong&gt;, then compare the p-value with a predefined threshold called &lt;em&gt;alpha&lt;/em&gt;, commonly assigned with 0.05, to decide whether to reject or not reject the null hypothesis.&lt;/p&gt;
&lt;img src=&#34;https://i.kym-cdn.com/entries/icons/original/000/018/489/nick-young-confused-face-300x256-nqlyaa.jpg&#34; alt=&#34;Confused&#34; width=&#34;300&#34;/&gt;
&lt;p&gt;&lt;em&gt;Source: &lt;a href=&#34;https://knowyourmeme.com/memes/confused-nick-young/photos&#34;&gt;https://knowyourmeme.com/memes/confused-nick-young/photos&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-does-it-actually-work&#34;&gt;How does it actually work?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s take a closer look at our performance rating example. Our null hypothesis states that there is no difference in ratings between departments, in order to test this hypothesis, we define the test statistic as the difference in the average ratings among different departments.&lt;/p&gt;
&lt;p&gt;To demonstrate the analysis, I generated 400 random ratings (continuous random values between 3 and 5) for 2 departments A and B with 200 records each in a way that ratings from department B are generally 0.2 higher than those of department A, then randomly drawn 100 records from each department and saved the dataset in a &lt;a href=&#34;rating_samples.csv&#34;&gt;csv file&lt;/a&gt;.&lt;/p&gt;
&lt;img src=&#34;csv_file_capture.png&#34; width=200/&gt;
&lt;p&gt;I am using &lt;a href=&#34;https://www.r-project.org/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R&lt;/a&gt; to help with the test given its simplicity, more detailed instructions can be found below if you would like to re-produce the analysis.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s load the dataset and use R&amp;rsquo;s &lt;code&gt;lm&lt;/code&gt; function to conduct the test.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(tidyverse)
library(broom)

# load the dataset
rating_df &amp;lt;- read_csv(&amp;quot;rating_samples.csv&amp;quot;)

# conduct the test
lm(rating ~ dept, rating_df) %&amp;gt;% tidy() %&amp;gt;% filter(term != &#39;(Intercept)&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;lm_result.png&#34; width=500/&gt;
&lt;p&gt;&lt;code&gt;lm&lt;/code&gt; function will use &lt;code&gt;dept&lt;/code&gt; A&amp;rsquo;s values as the baseline, each row is the result of the test whether there is a difference between a department&amp;rsquo;s ratings and the baseline department&amp;rsquo;s: if &lt;code&gt;p.value&lt;/code&gt; is less than the significance threshold, we can reject the null hypothesis and accept the alternative hypothesis that there is difference between the two groups of data.&lt;/p&gt;
&lt;p&gt;Given our &lt;code&gt;p.value&lt;/code&gt; for &lt;code&gt;deptB&lt;/code&gt; row is 0.000376, which is much lower than the significance threshold &lt;em&gt;alpha=0.05&lt;/em&gt;, we have enough evidence to reject the null hypothesis and accept the alternative hypothesis that there is indeed a difference between ratings of department A and B.&lt;/p&gt;
&lt;p&gt;Column &lt;code&gt;statistic&lt;/code&gt; is our test statistic: the mean of deptB&amp;rsquo;s ratings is at approximately 3.618, whilst column &lt;code&gt;term&lt;/code&gt; is the coefficient of department in a linear regression to predict that department&amp;rsquo;s &lt;code&gt;rating&lt;/code&gt;, simply put: when a person moves from department A to department B, their rating will likely increase by 0.281.&lt;/p&gt;
&lt;p&gt;Sounds like magic? I know!&lt;/p&gt;
&lt;p&gt;In case you want to re-create the analysis above, please follow the steps below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href=&#34;https://rstudio.com/products/rstudio/download/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RStudio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Set the working directory to where your analysis files to be: Session &amp;gt; Set Working Directory &amp;gt; Choose Directory &amp;hellip;&lt;/li&gt;
&lt;li&gt;Copy or create new &lt;code&gt;rating_samples.csv&lt;/code&gt; in the working directory, it should contain department name (&lt;code&gt;dept&lt;/code&gt;) and their employee ratings (&lt;code&gt;rating&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Copy and execute the script above in the Console panel&lt;/li&gt;
&lt;li&gt;If your &lt;code&gt;p.value&lt;/code&gt; for each department (&lt;code&gt;deptX&lt;/code&gt;) is less than 0.05, you can reject the null hypothesis and accept the alternative hypothesis: that department&amp;rsquo;s ratings are different from the baseline department&amp;rsquo;s&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The application of hypothesis testing in management is endless: we can use it to justify the investment in various marketing strategies or use it to validate the effectiveness of different product designs (A/B Testing) or use it to validate if there is a gender bias in recruitment, etc.&lt;/p&gt;
&lt;p&gt;There are actually different types of &lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hypothesis testing&lt;/a&gt;, each with different sets of &lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_assumption&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;assumptions&lt;/a&gt;, please do proper research before applying this test as using a wrong test will yield a misleading result.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Surviving a 7-Scrum-Team Project</title>
      <link>https://lephanthuymai.github.io/post/surving-a-7-Scrum-team-project/</link>
      <pubDate>Fri, 08 Jan 2021 14:36:15 +0700</pubDate>
      <guid>https://lephanthuymai.github.io/post/surving-a-7-Scrum-team-project/</guid>
      <description>&lt;p&gt;I was a Director of Delivery at an outsourcing firm when we got an opportunity to work with this client, let&amp;rsquo;s call them Company H. They would like to build a new generation restaurant management system as a software-as-a-service platform. My role was both Account Manager and Program Manager for this client, I gave the team&amp;rsquo;s overall direction, resolved cross-team impediments, reported the overall status and risks to the PMO and Steering Committee.&lt;/p&gt;
&lt;p&gt;The project started with 2 Scrum Teams then eventually grew to 7 Scrum Teams working on a &lt;strong&gt;single&lt;/strong&gt; product backlog.&lt;/p&gt;
&lt;p&gt;Just like other programs with multiple Scrum teams working on the same product backlog, we experienced the great challenge of dependency and integration hell, on top of that, we had 14-hour timezone difference between the team&amp;rsquo;s and their PO&amp;rsquo;s location, not to mention the challenges of outsourcing nature (tightly bound by contractual terms and sometimes lack of trust). In order to make our life easier, I chose to apply &lt;a href=&#34;https://www.scrum.org/scaled-professional-scrum-nexus-practices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nexus&lt;/a&gt; framework with adaptations to ensure multiple team&amp;rsquo;s works resulted in an integrated product. The model was not perfect but it was the best we could do at the time considering our constraints.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;Nexus Poster&#34;&gt;&lt;/p&gt;
&lt;p&gt;Each of our Scrum Team was composed of 1 Product Owner, 1 Scrum Master, 6 developers and 4 testers. Our Nexus Integration Team included an Onsite Coordinator (proxy of the PO), Nexus Scrum Master (rotating role), 3 Scrum Masters (each serving 2 Scrum Teams), 2 members from each Development Team, 1 Technical Architect, 1 Test Architect. At the program level, we had Steering Committee including both companies&#39; executives, Technical Governance and PMO to maintain transparency in communication and decision making.&lt;/p&gt;
&lt;p&gt;Each sprint was 2-week duration, we applied Sprint Planning, Daily Scrum for each Scrum team and Nexus Sprint Planning, Nexus Backlog Refinement, Nexus Daily Scrum, Nexus Sprint Review, Nexus Sprint Review at Nexus level.&lt;/p&gt;
&lt;p&gt;We conducted &lt;strong&gt;Nexus Backlog Refinement&lt;/strong&gt; meeting once every week, in which we had representatives from all Development teams and the PO to review the priority product backlog items and dependencies among them. The PO made initial team assignment for the reviewed product backlog items based on the teams&#39; discussion and dependencies. We utilized Google Spreadsheet retrieving data directly from JIRA API to make the planning easier, we could have used &lt;a href=&#34;https://www.atlassian.com/blog/portfolio-for-jira/portfolio-for-jira-3-and-beyond#:~:text=Portfolio%20for%20Jira%20helps%20you,on%20the%20same%20page%20throughout.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JIRA Portfolio&lt;/a&gt;, but it was more costly.&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;Nexus Sprint Planning&lt;/strong&gt; meeting, the Development teams sized the backlog items assigned to them, reviewed the plan in comparison to their availability for the new sprint, discuss with other teams and the PO on their plan before finalizing. There was one important outcome missing: we did not define the Nexus Sprint Goal as we did not see how important it was back then.&lt;/p&gt;
&lt;p&gt;We had &lt;strong&gt;Nexus Daily Scrum&lt;/strong&gt; with representatives from all Development teams to discuss on integration issues, the meetings were very helpful to early identify and address integration challenges, but they were also a pain given a lot of people involved, we tried different approaches to simplify and make these meetings more efficient, eventually we reached to a point when it took less than 1 hour but were not able to reduce it further .&lt;/p&gt;
&lt;p&gt;The team applied &lt;strong&gt;Gitflow with feature branch&lt;/strong&gt; workflow, when a feature was completed (developed and tested), it would be merged into the dev branch and built automatically to a test environment for the PO to early review and feedback. We had some automated end-end regression test suites to validate integrated workflows and 60-80% unit test (branch) coverage for all code, which helped to save some testing time, but given the early stage of the product with ever-changing features and tight deadline, the majority of the testing was done manually including scripted tests and exploratory tests.&lt;/p&gt;
&lt;p&gt;We had &lt;strong&gt;weekly PMO meetings&lt;/strong&gt; to report the progress which was visualized using Sprint burn-down chart and Release burn-down chart. Risks were also raised and monitored in those meetings.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Nexus Sprint Review&lt;/strong&gt; was hosted by the PO, the teams demonstrated what was done, received feedback from the stakeholders, which would be added to the backlog for next sprints. The meeting was recorded and shared across the organization so that everyone was aware of the progress.&lt;/p&gt;
&lt;p&gt;The Teams conducted &lt;strong&gt;Nexus Sprint Retrospective&lt;/strong&gt; together, then Sprint Retro for each team, feedback/action items were shared among the teams and PO. We applied various practices as Lean Coffee or simple brainstorming technique on Start/Stop/Continue, sometimes the SMs got creative and asked the teams to do sentimental drawings or arranged team outing to create a comfortable environment to share their ideas.&lt;/p&gt;
&lt;p&gt;Overall, the processes we applied were helpful, we demonstrated well our capability in a fast-pace project and won much more business with the client, our team size grew from 20-ish engineers to more than 100 engineers at some point, but that was not without challenges, I will write more about our lesson learn, especially in release management in the upcoming posts.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
